{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHANES Variable Embeddings with BioBERT\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading NHANES 2017-2018 variable metadata\n",
    "2. Encoding variable descriptions with BioBERT\n",
    "3. Visualizing variables in 3D semantic space using UMAP\n",
    "\n",
    "The metadata includes all variables from 5 categories:\n",
    "- Demographics\n",
    "- Dietary\n",
    "- Examination\n",
    "- Laboratory\n",
    "- Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Import our simple NHANES fetcher\n",
    "from simple_nhanes_fetcher import SimpleNHANESFetcher\n",
    "\n",
    "# BioBERT and embeddings\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fetch NHANES 2017-2018 Variable List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fetcher\n",
    "fetcher = SimpleNHANESFetcher()\n",
    "\n",
    "# Fetch all variables for 2017-2018 cycle\n",
    "df_variables = fetcher.fetch_all_for_cycle(\"2017-2018\")\n",
    "\n",
    "print(f\"\\nTotal variables: {len(df_variables)}\")\n",
    "print(f\"\\nColumns: {df_variables.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_variables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution across components\n",
    "print(\"Variables per component:\")\n",
    "print(df_variables['component'].value_counts())\n",
    "\n",
    "# Sample variables\n",
    "print(\"\\nSample variables:\")\n",
    "print(df_variables[['variable_name', 'variable_description', 'component']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load BioBERT Model\n",
    "\n",
    "We'll use BioBERT, a biomedical language model pre-trained on PubMed and PMC articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BioBERT model and tokenizer\n",
    "model_name = \"dmis-lab/biobert-v1.1\"  # BioBERT base model\n",
    "\n",
    "print(f\"Loading BioBERT model: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encode Variable Descriptions with BioBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(texts, model, tokenizer, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    Encode texts using BioBERT.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        model: BioBERT model\n",
    "        tokenizer: BioBERT tokenizer\n",
    "        device: torch device\n",
    "        batch_size: Batch size for encoding\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings (n_texts, embedding_dim)\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            # Use [CLS] token embedding (first token)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        \n",
    "        embeddings.append(batch_embeddings)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + len(batch_texts)}/{len(texts)} texts\")\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "print(\"Encoding function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare texts for encoding\n",
    "# Combine variable name and description for richer context\n",
    "texts = [\n",
    "    f\"{row['variable_name']}: {row['variable_description']}\"\n",
    "    for _, row in df_variables.iterrows()\n",
    "]\n",
    "\n",
    "print(f\"Encoding {len(texts)} variable descriptions...\")\n",
    "embeddings = encode_text(texts, model, tokenizer, device, batch_size=32)\n",
    "\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "\n",
    "# Add embeddings to dataframe (store as separate numpy file to save memory)\n",
    "np.save('nhanes_2017_2018_embeddings.npy', embeddings)\n",
    "print(\"\\nSaved embeddings to: nhanes_2017_2018_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality Reduction with UMAP\n",
    "\n",
    "Reduce from 768 dimensions (BioBERT embedding size) to 3D for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions for visualization\n",
    "print(\"Reducing dimensions with UMAP...\")\n",
    "\n",
    "# UMAP to 3D\n",
    "reducer_3d = UMAP(\n",
    "    n_components=3,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "coords_3d = reducer_3d.fit_transform(embeddings)\n",
    "\n",
    "print(f\"3D coordinates shape: {coords_3d.shape}\")\n",
    "\n",
    "# Add to dataframe\n",
    "df_variables['x'] = coords_3d[:, 0]\n",
    "df_variables['y'] = coords_3d[:, 1]\n",
    "df_variables['z'] = coords_3d[:, 2]\n",
    "\n",
    "print(\"\\nDimensionality reduction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive 3D Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    df_variables,\n",
    "    x='x',\n",
    "    y='y',\n",
    "    z='z',\n",
    "    color='component',\n",
    "    hover_data=['variable_name', 'variable_description', 'data_file_name'],\n",
    "    title='NHANES 2017-2018 Variables in 3D Semantic Space (BioBERT + UMAP)',\n",
    "    labels={'component': 'Component'},\n",
    "    width=1000,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_traces(\n",
    "    marker=dict(size=3, opacity=0.7),\n",
    "    hovertemplate='<b>%{customdata[0]}</b><br>' +\n",
    "                  '%{customdata[1]}<br>' +\n",
    "                  'File: %{customdata[2]}<br>' +\n",
    "                  '<extra></extra>'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='UMAP 1',\n",
    "        yaxis_title='UMAP 2',\n",
    "        zaxis_title='UMAP 3'\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save as HTML\n",
    "fig.write_html('nhanes_2017_2018_variables_3d.html')\n",
    "print(\"Saved visualization to: nhanes_2017_2018_variables_3d.html\")\n",
    "\n",
    "# Display\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 2D Visualization (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also create a 2D version for easier viewing\n",
    "print(\"Creating 2D projection...\")\n",
    "\n",
    "reducer_2d = UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "coords_2d = reducer_2d.fit_transform(embeddings)\n",
    "\n",
    "df_variables['x_2d'] = coords_2d[:, 0]\n",
    "df_variables['y_2d'] = coords_2d[:, 1]\n",
    "\n",
    "# Create 2D scatter plot\n",
    "fig_2d = px.scatter(\n",
    "    df_variables,\n",
    "    x='x_2d',\n",
    "    y='y_2d',\n",
    "    color='component',\n",
    "    hover_data=['variable_name', 'variable_description'],\n",
    "    title='NHANES 2017-2018 Variables in 2D Semantic Space (BioBERT + UMAP)',\n",
    "    labels={'component': 'Component'},\n",
    "    width=1200,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig_2d.update_traces(\n",
    "    marker=dict(size=5, opacity=0.6)\n",
    ")\n",
    "\n",
    "fig_2d.write_html('nhanes_2017_2018_variables_2d.html')\n",
    "print(\"Saved 2D visualization to: nhanes_2017_2018_variables_2d.html\")\n",
    "\n",
    "fig_2d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Semantic Search Example\n",
    "\n",
    "Find variables semantically similar to a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_variables(query, df_variables, embeddings, model, tokenizer, device, top_k=10):\n",
    "    \"\"\"\n",
    "    Find variables similar to a query using semantic search.\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = encode_text([query], model, tokenizer, device, batch_size=1)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    # Return results\n",
    "    results = df_variables.iloc[top_indices].copy()\n",
    "    results['similarity'] = similarities[top_indices]\n",
    "    \n",
    "    return results[['variable_name', 'variable_description', 'component', 'similarity']]\n",
    "\n",
    "# Example: Search for cholesterol-related variables\n",
    "print(\"Searching for variables similar to 'cholesterol'...\\n\")\n",
    "results = find_similar_variables(\n",
    "    \"cholesterol levels in blood\",\n",
    "    df_variables,\n",
    "    embeddings,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example: diabetes-related variables\n",
    "print(\"Searching for variables similar to 'diabetes'...\\n\")\n",
    "results = find_similar_variables(\n",
    "    \"diabetes glucose blood sugar\",\n",
    "    df_variables,\n",
    "    embeddings,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the enriched dataframe\n",
    "df_variables.to_csv('nhanes_2017_2018_variables_with_coords.csv', index=False)\n",
    "print(\"Saved enriched variable list to: nhanes_2017_2018_variables_with_coords.csv\")\n",
    "\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - nhanes_2017_2018_embeddings.npy (BioBERT embeddings)\")\n",
    "print(\"  - nhanes_2017_2018_variables_with_coords.csv (Variables + 2D/3D coordinates)\")\n",
    "print(\"  - nhanes_2017_2018_variables_3d.html (Interactive 3D visualization)\")\n",
    "print(\"  - nhanes_2017_2018_variables_2d.html (Interactive 2D visualization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Fetching NHANES 2017-2018 variable metadata from CDC\n",
    "2. ✅ Encoding ~2000-3000 variables with BioBERT\n",
    "3. ✅ Reducing to 2D/3D using UMAP\n",
    "4. ✅ Creating interactive visualizations\n",
    "5. ✅ Semantic search for similar variables\n",
    "\n",
    "### Next Steps:\n",
    "- Use `nhanes_data_loader_all_years.py` to download actual NHANES data (not just metadata)\n",
    "- Merge data across years by SEQN\n",
    "- Export to CSV or Google Drive\n",
    "- Use embeddings for variable selection in hypothesis generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
